q()
rating <- myHTML %>% html_nodes(".ratings-imdb-rating strong") %>% html_text()
movies <- data.frame(name,year,cast,genre,rating, stringsAsFactors = FALSE)
install.packages("rvest")
install.packages("rvest")
folder <- "C:/scraped_html"
setwd(folder)
link <- "https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc.html"
link %>%
read_html() -> myHTML
myHTML %>%
write_html("IMDB_top100.html")
name <- myHTML %>% html_nodes(".lister-item-header a") %>% html_text()
install.packages("rvest")
library("rvest")
install.packages("rvest")
install.packages("magrittr")
install.packages("jsonlite")
folder <- "C:/scraped_html"
setwd(folder)
install.packages("rvest")
install.packages("magrittr")
install.packages("jsonlite")
install.packages("R2HTML")
install.packages("xml2")
library("xml2")
library("R2HTML")
library("rvest")
library("magrittr")
library("jsonlite")
library("dplyr")
library("xml2")
library("R2HTML")
library("rvest")
library("magrittr")
library("jsonlite")
library("dplyr")
folder <- "C:/scraped_html"
setwd(folder) # it sets working directory as folder
folder <- "C:/scraped_html"
setwd(folder) # it sets working directory as folder
setwd(folder)
folder <- "C:/scraped_html"
setwd(folder) #
install.packages("rvest")
install.packages("magrittr")
install.packages("jsonlite")
install.packages("R2HTML")
install.packages("xml2")
library("xml2")
library("R2HTML")
library("rvest")
library("magrittr")
library("jsonlite")
library("dplyr")
install.packages("rvest")
install.packages("magrittr")
install.packages("jsonlite")
install.packages("R2HTML")
install.packages("xml2")
library("xml2")
library("R2HTML")
library("rvest")
library("magrittr")
library("jsonlite")
library("dplyr")
folder <- "C:/scraped_html"
setwd(folder)
setwd(folder)
before2000 <- c("TRUE", "FALSE")
if(year < 2000){
before2000 = "TRUE"
}
else{
before2000 = "FALSE"
}
movies <- data.frame(name,year,cast,genre,rating,before2000, stringsAsFactors = FALSE)
View(movies)
install.packages("rvest")
install.packages("magrittr")
install.packages("jsonlite")
install.packages("R2HTML")
install.packages("xml2")
install.packages("gmodels")
install.packages("dplyr")
install.packages("caret", dependencies = TRUE)
library("xml2")
library("R2HTML")
library("rvest")
library("magrittr")
library("jsonlite")
library("dplyr")
# scraped_html isimli bir dosyayı masaüstünden oluşturun ve onun lokasyonunu "" içerisine yazıp foldera eşitleyin
folder <- "C:/scraped_html"
setwd(folder) # it sets working directory as folder
#websitesinin linki
link <- "https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc.html"
link %>%
read_html() -> myHTML
myHTML %>%
write_html("IMDB_top100.html")
#sitedeki name,year,cast,genre ve ratingleri çeker
name <- myHTML %>% html_nodes(".lister-item-header a") %>% html_text()
year <- myHTML %>% html_nodes(".text-muted.unbold") %>% html_text()
cast <- myHTML %>% html_nodes(".text-muted+ p") %>% html_text()
genre <- myHTML %>% html_nodes(".genre") %>% html_text()
rating <- myHTML %>% html_nodes(".ratings-imdb-rating strong") %>% html_text()
#çekilen veriler data frame de birleştirilir
movies <- data.frame(name,year,cast,genre,rating, stringsAsFactors = FALSE)
#View komutu ile veriseti gösterilir
View(movies)
length(which(!complete.cases(movies))) # there is no missing data
# 2) Data Preprocessing // movie kismina plot eklenip name yerine stemming daha efektif kullanilabilir
movies$name
library("caret")
set.seed(44186) # for reproducability
sample.size <- floor(0.80 * nrow(movies)) # %80 for training, %20 for testing
train.index <- sample(seq_len(nrow(movies)), size = sample.size)
train <- movies[train.index, ]
test <- movies[-train.index, ]
View(train)
View(test)
# verify proportions
prop.table(table(train$name))
prop.table(table(test$name))
install.packages("quanteda")
library("quanteda")
# making tokenization on names
train.tokens <- tokens(train$name, what = "word", remove_numbers = TRUE,remove_punct = TRUE,remove_symbols = TRUE, remove_separators = TRUE)
# see the change of an example
train.tokens[[1]]
#lowercase the tokens
train.tokens <- tokens_tolower(train.tokens)
train.tokens[[1]]
#stopwords
train.tokens <- tokens_select(train.tokens,stopwords(),selection = "remove")
train.tokens[[1]]
stopwords()
#stemming
train.tokens <- tokens_wordstem(train.tokens,language = "english")
train.tokens[[9]] # real one is saving private ryan
# bag of words model creation
train.tokens.dfm <- dfm(train.tokens,tolower = FALSE)# document feature matrix
#transfroming to a matrix
train.tokens.matrix <- as.matrix(train.tokens.dfm)
View(train.tokens.matrix[1:20,1:40])
# dimension
dim(train.tokens.matrix) # 40 doc 77 column
#observing the effects of stemming
colnames(train.tokens.matrix)[1:40]
#cross validation which is used for useing the train data most efficient way
# making feature based data frame
x <- convert(train.tokens.dfm, to = "data.frame")
train.tokens.df <- cbind(name = train$name, x)
#Clean up the column names
names(train.tokens.df) <- make.names(names(train.tokens.df))
#creating stratified folds for 10-fold cross validation repeated
set.seed(44186)
cross_validation.folds <- createMultiFolds(train$name, k = 10, times = 3)
cross_validation.cntrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3, index = cross_validation.folds)
# creating a function for calculating the term frequency
term.frequency <- function(row){
row / sum(row)
}
# creating a function for calculating inverse document frequency
inverse.document.frequency <- function(col){
corpus.size <- length(col)
document.count <- length(which(col > 0))
log10(corpus.size / document.count)
}
# creating a function for calculating term frequency and inverse document frequency
tf.idf <- function(t,idf){
t * idf
}
# normalize all docs by term frequency , apply: function that applies functions to matrixes
train.tokens.df <- apply(train.tokens.matrix, 1 , term.frequency)
dim(train.tokens.df)
View(train.tokens.df[1:20,1:40]) # in last view function, the values are 1 but now values are 0.5
train.tokens.idf <- apply(train.tokens.matrix,2,inverse.document.frequency)
str(train.tokens.idf)
train.tokens.term_freq_inverse_document_freq <- apply(train.tokens.df,2, tf.idf, idf = train.tokens.idf)
dim(train.tokens.term_freq_inverse_document_freq)
View(train.tokens.term_freq_inverse_document_freq[1:20,1:20])
train.tokens.term_freq_inverse_document_freq <- t(train.tokens.term_freq_inverse_document_freq)
dim(train.tokens.term_freq_inverse_document_freq) # 40 docs and each one has 77 columns
View(train.tokens.term_freq_inverse_document_freq[1:20,1:20])
incomp.cases <- which(!complete.cases(train.tokens.term_freq_inverse_document_freq))
train$name[incomp.cases] # 0 incomplete cases
train.tokens[[4]]
train.tokens <- tokens_ngrams(train.tokens, n = 1:2) # bigram
train.tokens[[4]]
train.tokens.dfm <- dfm(train.tokens,tolower = FALSE)
train.tokens.matrix <- as.matrix(train.tokens.dfm)
train.tokens.dfm # by adding bi-grams, the number of columns in our matrix has has increased almost 2x
train.tokens.df <- apply(train.tokens.matrix,1,term.frequency)
train.tokens.idf <- apply(train.tokens.matrix,2,inverse.document.frequency)
#tf-idf calculation
train.tokens.term_freq_inverse_document_freq <- apply(train.tokens.df,2, tf.idf, idf = train.tokens.idf)
train.tokens.term_freq_inverse_document_freq <- t(train.tokens.term_freq_inverse_document_freq)
# garbage collection for cleaning unused obj
gc()
install.packages("irlba")
library("irlba") # Find a few approximate singular values and corresponding singular vectors of a matrix.
train.irlba <- irlba(t(train.tokens.term_freq_inverse_document_freq), nv = 20 , maxit = 76 )
# total.time <- Sys.time() - start.time
#total.time
# observing new feature of data
View(train.irlba$v) # v = approximate right singular vectors
library("caret")
install.packages("randomForest")
library("randomForest")
library("mlbench")
ifelse( year < 2000, "TRUE", "FALSE")
set.seed(123) # used for reproduce random numbers
data <- data.frame(Actual = sample(c("True","False"), 100, replace = TRUE),
Prediction = sample(c("True","False"), 100, replace = TRUE))
library(caret)
confusionMatrix(as.factor(data$Prediction), as.factor(data$Actual), positive = "True")
library(caTools)
library(randomForest)
train$year <- as.factor(train$year)
str(train)
set.seed(120)  # Setting seed
classifier_RF = randomForest(x = train[-6], #we write (-6) bcs we have 5 variables
y = train$year,
ntree = 500) #number of trees
classifier_RF
y_pred = predict(classifier_RF, newdata = train[-5])
y_pred
y_pred = predict(classifier_RF, newdata = train[-6])
y_pred
plot(classifier_RF)
importance(classifier_RF)
varImpPlot(classifier_RF)
library(ggplot2)
qplot(year, rating, data = train)
str(train)
library(e1071)
model <- svm(year~., data= train)
summary(model)
